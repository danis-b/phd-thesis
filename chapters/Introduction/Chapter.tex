\chapter{Introduction}\label{ch:intro}

Tradeoffs pervade human decision-making, from everyday choices to complex scientific problems. Consider housing: living near the city center typically means higher rent for a smaller space, but offers shorter commutes, while living in the suburbs offers more space at lower cost, but requires longer commutes. Similarly, the choice between cooking and eating out balances time, health, and financial considerations: cooking at home is generally healthier and more economical, but requires time for planning and preparation, while eating out offers convenience at the expense of money and health.

Tradeoffs are particularly fundamental to physics and computer science, where they shape our approach to solving complex problems. In physics, these tradeoffs manifest themselves as approximations. Mean-field theory~\cite{Bruus2004ManyBody} provides a canonical example, replacing many-body electron-electron interactions with an average effective potential experienced by each electron. This transforms an intractable many-body problem into separable single-particle problems that are easy to solve. The tight-binding approximation~\cite{Kittel2005IntroductionTo,Vonsov2012QuantumSolidS} assumes that electrons are tightly bound to atoms and can only jump between nearby atoms (often just nearest or next nearest neighbors), expressing the electronic wave functions as linear combinations of atomic orbitals. While this provides a natural discretization that makes the problem finite-dimensional, it limits the expressibility of the wave function ansatz. Another example is the Born-Oppenheimer approximation, which decouples electronic and nuclear motion based on their mass difference. Since nuclei are much more massive than electrons, they move more slowly, allowing electronic states to be treated as instantaneously adapting to nuclear positions. This separation dramatically simplifies the many-body Schr√∂dinger equation, but fails to capture important phenomena such as the Jahn-Teller effect~\cite{Vonsov2012QuantumSolidS}. The art of theoretical physics is to use physical intuition to choose approximations that make problems tractable while preserving the essential physical phenomena of interest.

Similar tradeoffs occur in computer science and software engineering. For instance, the Hindley-Milner type system~\cite{Hindle_1969_The_Principal_T,Milner_1978_A_theory_of_typ} offers complete type inference without programmer annotations but precludes dependent types and more expressive type systems. In numerical computing, choosing 16-bit over 64-bit floating-point numbers allows efficient use of specialized hardware such as Intel's tile registers~\cite{Intel_AMX} and NVIDIA's tensor cores~\cite{NvidiaTensorCores}, significantly improving the performance of some workloads at the expense of numerical precision and code complexity.

Getting too comfortable with certain approximations can limit our conceptual framework, creating intellectual blind spots that hinder innovation. Learning from other fields helps overcome these limitations, as demonstrated by the growth of computational physics. Although the field is over 80 years old, it remains very active and continues to influence both computer science and theoretical physics. Recent winners of the Gordon Bell Prize show how physics problems are driving advances in high-performance computing (HPC)~\cite{Stocks_2024_Breaking_the_Mi,Das_S_2023_Large_Scale_Mat,Fedeli_2022_Pushing_the_Fro,Liu_Y_2021_Closing_the_qu}. The impact flows in both directions, with breakthroughs in computing gaining recognition in physics and other natural sciences---DeepMind's AlphaFold2~\cite{Jumper_2021_Highly_accurate} won the 2024 Nobel Prize in Chemistry for solving the problem of protein folding~\bibnote{Press release at \url{https://www.nobelprize.org/prizes/chemistry/2024/press-release/}}, while Geoffrey Hinton and John J. Hopfield shared the 2024 Nobel Prize in Physics~\bibnote{Press release at \url{https://www.nobelprize.org/prizes/physics/2024/press-release/}} for their ``foundational discoveries and inventions that enable machine learning with artificial neural networks''. The creation of dedicated venues for computational physics software, such as the SciPost Physics Codebases journal~\bibnote{\url{https://scipost.org/SciPostPhysCodeb}}, reflects a fundamental shift: the development of new numerical methods and software has become as essential to theoretical physics as traditional analytical approaches.

The computing landscape has been dramatically reshaped by unprecedented investments in machine learning (ML) in recent years, with hundreds of billions of dollars driving innovation not only in deep learning models and algorithms, but also in the underlying computing infrastructure, programming languages, and software frameworks. While much attention has focused on large language models (LLMs) such as OpenAI's GPT-4~\cite{OpenAI_2023_GPT_4_Technical} or Google's Gemini~\cite{Team_2023_Gemini_A_Famil}, the same companies have developed tools that are potentially more relevant to computational physics, such as the Triton~\cite{Tillet_2019_Triton_an_inte} and Pallas languages~\bibnote{\url{https://docs.jax.dev/en/latest/pallas/index.html}} for programming accelerators. As the gap between state-of-the-art computing methods and those commonly used in computational physics widens, it becomes increasingly important to investigate how modern computer science and software engineering techniques can improve our ability to solve complex physics problems. This thesis addresses this challenge by investigating several key questions: How can we take advantage of recent developments in programming languages and compilation techniques to simplify and accelerate physics computations? How can we ensure that our results remain reproducible? By carefully investigating these questions, we aim to demonstrate how bridging the gap between modern computing advances and computational physics can lead to significant improvements in both performance and scientific reliability.

This thesis focuses on a particular area of physics---quantum many-body physics. The following two sections outline some advances in computational methods and hardware that I believe are particularly relevant today, and then discuss which problems in quantum many-body physics can benefit from these advances.

\section{Exciting developments in the world of computing}

\subsection{Data-oriented design}\label{sec:intro:data-oriented}

The hardware landscape of modern computing systems is dominated by a growing disparity between processing and memory access speeds, known as the "memory wall." This phenomenon, first described in 1994~\cite{Wulf_1995_Hitting_the_mem}, is illustrated in Figure~\ref{fig:in:memory-wall}. Figure~\ref{fig:in:nvidia-gpus} provides additional data for more recent Nvidia GPUs. Although processor frequencies have not increased over the past decade, processing power has grown exponentially due to increased parallelism. This parallelism has been achieved partly through increased processor core counts and partly through specialized registers. For instance, modern CPUs contain single instruction multiple data (SIMD) registers that operate directly in vectors instead of scalars and can perform up to 32 floating point operations in parallel within a single cycle. More recently, Nvidia, AMD, and Intel added registers that operate directly on matrices. They are called tensor cores in Nvidia GPUs~\cite{NvidiaTensorCores}, matrix cores in AMD GPUs~\bibnote{\url{https://www.amd.com/en/technologies/cdna.html}}, and tile registers in Intel CPUs~\cite{Intel_AMX}, and, for instance, an AMD MI250X GPU can perform 1024 floating point operations per cycle within one such matrix core.

Historically, efforts to mitigate the memory wall began long before its formal identification. In the 1960s, memory systems began evolving into hierarchical structures with special memory systems located very close to (or directly on) the processor, so-called caches~\cite{Liptay_1968_Structural_aspe}. These caches are much faster to access than main memory, but are very limited in size. Initially, CPUs had only one cache level, L1. Today, most processors have at least three cache levels: L1, L2, and L3. Although caches help hide some memory access latencies, they cannot eliminate them completely, and memory bandwidth remains the limiting factor. Due to fundamental engineering constraints, memory bandwidth has not kept pace with the growth in processors' computing capabilities. 

\normalcaptionwidth
\captionwidth{\linewidth}
\begin{figure}[h]
    \centering
    \input{./assets/Figure_1.pgf}%

    \hspace{0.025\textwidth}%
    \begin{minipage}[t]{0.42\textwidth}
      \centering
      \innerFigCaption{Comparison between CPU and RAM performance growth. This figure illustrates the ``memory wall'' phenomenon, where although both CPU and RAM have improved exponentially, processor computing capabilities have advanced much faster than memory access speeds. Data is reproduced from~\cite{Hennes_2012_Computer_archit} measured using the Standard Performance Evaluation Corporation (SPEC) benchmark.}%
      \label{fig:in:memory-wall}
    \end{minipage}%
    \hspace{0.03\textwidth}%
    \begin{minipage}[t]{0.42\textwidth}
      \centering
      \innerFigCaption{Comparison between computing performance and memory bandwidth for the last 5 generations of Nvidia datacenter GPUs. Data is presented for the Tesla (P100), Volta (V100), Hopper (H100) and Blackwell architectures. The performance of P100 is taken as a starting point. Increase in 16-bit float performance (without sparsity) is compared to the increase in the bandwidth of the high bandwidth memory (HBM).}%
      \label{fig:in:nvidia-gpus}
    \end{minipage}%
\end{figure}
\changecaptionwidth
\captionwidth{0.9\linewidth}

Operations are typically classified as either compute-bound, where performance is limited by processing speed, or memory-bound, where memory access time is the limiting factor. With memory bandwidth growing slower than processing power, many algorithms that were previously compute-bound are becoming memory-bound. This shift forces us to rethink traditional approaches to algorithm design and implementation. A clear example of this evolution can be found in the implementation of trigonometric functions. While older software, like the DOOM video game (1993)~\bibnote{\url{https://en.wikipedia.org/wiki/Doom_\%281993_video_game\%29}} used look-up tables (LUTs)~\bibnote{\url{https://github.com/id-Software/DOOM/blob/a77dfb96cb91780ca334d0d4cfd86957558007e0/linuxdoom-1.10/tables.c#L20}} for sine, tangent, and inverse tangent functions, modern implementations rely on polynomial interpolation~\bibnote{\url{https://github.com/shibatch/sleef/blob/d1663ca43e58bdcdc93afcca94692cd571dd8c54/src/libm/sleefsimdsp.c#L630}}. Polynomial coefficients take up much less memory than a full look-up table, fit into processor caches, and are thus much faster to retrieve than the table entries. In addition, computing sines of, e.g., eight different angles required eight separate table look ups, whereas by using SIMD registers, the polynomial can be evaluated for eight different angles simultaneously. Despite the additional computation involved in polynomial evaluation, these new algorithms turn out to be faster, while also achieving higher accuracy.

The impact of the memory wall extends to cutting-edge machine learning applications. In 2020, when training a popular language model called BERT~\cite{Devlin_2018_BERT_Pre_train}, just 1\% of floating-point operations took up nearly 40\% of the runtime because they were memory-bound~\cite{Ivanov_2020_Data_Movement_I}. This led to widespread adoption of kernel fusion, a technique that combines memory-bound operations with compute-bound operations to maintain overall compute-bound performance. In deep learning, a common pattern is a linear operation (matrix multiplication) followed by non-linear activation function such as ReLU (Rectifier Linear Unit). These operations are called \emph{kernels} because they have very efficient implementations and serve as the building blocks in neural networks. When ReLU is applied elementwise to a vector, the overhead of loading the input and storing the result to memory outweighs the computation itself. However, by fusing the kernels and applying ReLU directly after the matrix multiplication while the data is still in registers or cache, memory access is minimized and performance improves.

An important consequence of the memory wall is that computational complexity, as commonly taught in algorithms and data structures courses, does not accurately predict real-world performance. To see an example of this, consider two algorithms: a matrix-vector (MV) multiplication $c = A b$ and a matrix-matrix (MM) multiplication $C = A B$ (where $b,\,c\in \mathbb{R}^N$ and $A,\,B,\,C\in \mathbb{R}^{N\times N}$). These algorithms are very similar and, in fact, the matrix-matrix multiplication can be expressed as a sequence of matrix-vector products $C_i = A B_i$, where $C_i$ and $B_i$ are the $i$'th columns of $C$ and $B$, respectively. Or in pseudocode:
\begin{minted}[escapeinside=||,mathescape=true]{text}
def matrix_matrix_product(A, B):
  for i in range(N):
    for j in range(N):               #
      acc = 0                        # |Matrix-vector product $C_i = A B_i$|.
      for k in range(N):             # 
        acc += A[j, k] * B[k, i]     #
      C[j, i] = acc
  return C
\end{minted}
The computational complexity of every matrix-vector (MV) product $C_i = A B_i$ is $\mathcal{O}(N^2)$, and the computational complexity of the matrix-matrix product is $\mathcal{O}(N^3)$. Let us compare the performance of the MV and MM algorithms. Figure~\ref{fig:in:gemm-flops} shows the number of floating point operations per second (FLOP/s) that state-of-the-art implementations of each algorithm achieve. For large $N$, the matrix-matrix multiplication is about 100 times more performant! That means that if we were to implement the MM product as a sequence of MV products, effectively computing the identical number of FLOPs, the code would run 100 times slower. The fastest implementations of the MM algorithm have $\mathcal{O}(N^3)$ computational complexity, just like a sequence of MV products, so why do they perform so much better? The explanation is simple: a matrix-vector multiplication performs $\mathcal{O}(N^2)$ floating point operations and accesses $\mathcal{O}(N^2)$ bytes, so it is memory-bound (for large enough $N$ when the data does not all fit into registers or cache). In fact, the MV algorithm in Figure~\ref{fig:in:gemm-flops} achieves close to theoretical memory bandwidth. In contrast, when considering the MM algorithm, there are $\mathcal{O}(N^3)$ floating point operations, but only $\mathcal{O}(N^2)$ bytes in input and output matrices. That means that each byte is reused in $\mathcal{O}(N)$ operations, and by cleverly ordering the operations, the algorithm can be made compute-bound for sufficiently large $N$, and this is precisely what the best implementations do.

% A good example of this phenomenon is the difference
% A striking example is matrix-matrix multiplication, which is one of the most optimized algorithms in computing, with good implementations routinely achieving over 95\% of theoretical hardware performance. Mathematically, a matrix-matrix multiplication $C = A B$ can be expressed as a sequence of matrix-vector products $C_i = A B_i$, where $C_i$ and $B_i$ are the i'th columns of $C$ and $B$, respectively. This is equivalent to an implementation with three nested loops:
% For square matrices, the computational complexity of the sequence of matrix-vector products is $\mathcal{O}(N\cdot N^2) = \mathcal{O}(N^3)$.
% Modern matrix-matrix multiplication implementations process data in tiles (smaller rectangular blocks of data) and have no fewer than six nested loops, but their computational complexity is also $\mathcal{O}(N^3)$. The real-world performance of the two algorithms is drastically different, as shown in Figure~\ref{fig:in:gemm-flops}. The implementation using matrix-vector products performs significantly worse. The explanation is simple: each matrix-vector multiplication performs $\mathcal{O}(N^2)$ floating point operations while accessing $\mathcal{O}(N^2)$ bytes, so it is memory-bound. In contrast, when considering the full matrix-multiplication algorithm, there are $\mathcal{O}(N^3)$ floating point operations, but only $\mathcal{O}(N^2)$ bytes in input and output matrices. That means that each byte is reused in $\mathcal{O}(N)$ operations, and by cleverly ordering the operations, the algorithm can be made compute-bound for sufficiently large $N$, and this is precisely what modern implementations do.

\normalcaptionwidth
\captionwidth{\linewidth}
\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.4\textwidth}%
      \vspace{-\fboxsep}%
      \input{./assets/Figure_2.pgf}
    \end{minipage}%
    \hspace{0.05\textwidth}%
    \begin{minipage}[t]{0.47\textwidth}%
      \vspace{5pt}%
      \centering%
      \innerFigCaption{Performance comparison of a square matrix-matrix product (MM) and matrix-vector product (MV) on an Nvidia V100S PCIe. Performance is measured by the number of floating point operations per second, FLOP/s. $\text{TFLOP/s} = 10^{12}\,\text{FLOP/s}$. ``f16'' denotes 16-bit floating point numbers, and ``f32''---32-bit. MM f16 operation is able to utilize tensor cores and achieves an order of magnitude higher performance than MM f32.}%
      \label{fig:in:gemm-flops}
    \end{minipage}%
\end{figure}
\changecaptionwidth
\captionwidth{0.9\linewidth}

Modern hardware design provides clear guidelines for achieving optimal performance. Consider the NVIDIA H100 GPU: 94\% of its theoretical performance comes from tensor cores~\bibnote{\url{https://resources.nvidia.com/en-us-grace-cpu/h100-datasheet-24306-1}}, specialized hardware units that perform matrix-matrix multiplications on small tiles (about $16\times 16$ elements). GPUs, like other processors, operate in cycles where a new instruction is issued every 0.5--1 ns (ignoring details where multiple instructions may be issued in parallel). On the H100, any cycle that does not utilize tensor cores through MMA (matrix multiply-accumulate) operations, accesses at most 6\% of the available hardware resources. Therefore, the art of achieving high performance lies in structuring data to allow direct operation by these specialized hardware units.

\subsection{Programming languages}

Understanding algorithms in terms of data transformations is necessary, but not sufficient for practical implementation. One must express these algorithms in a form that hardware can execute, which is achieved through programming languages. The landscape of modern programming languages is diverse, with each language embodying different design philosophies, implementation approaches, and intended use cases. This section examines some recent developments in programming languages that are particularly relevant to numerical computing and computational physics.

An interesting trend over the past decade has been the convergence of data science, machine learning, and high-performance computing (HPC) towards array programming paradigms. Data science has embraced so-called column-oriented databases for analytical queries, starting with proprietary solutions such as KDB+/Q~\bibnote{\url{https://kx.com/products/kdb/}} and evolving to modern formats such as Apache Arrow~\bibnote{\url{https://arrow.apache.org/}}, database engines such as DuckDB~\bibnote{\url{https://duckdb.org/}}, and libraries such as Pandas~\cite{reback2020pandas,McKinn_2010_Data_Structures}. These systems are fundamentally built around highly optimized operations on one-dimensional arrays (vectors). Similarly, the machine learning community has recognized that deep learning research primarily requires a good tensor library with automatic differentiation support. In the HPC domain, while experts focus on optimizing remote memory access and overlapping computation with communication, the core computations are typically performed by optimized tensor library routines. This convergence has established a paradigm where complex computations are constructed by assembling pre-optimized operations (kernels) acting on arrays, much like building models from elementary LEGO\textregistered\   bricks.

Several aspects are important in this new landscape: ensuring correctness, integrating custom low-level kernels, and effectively exploiting internode parallelism. Two distinct approaches to code development have emerged: the iterative, experiment-driven approach exemplified by the run-eval-print-loop (REPL), and the more formal, ``think-first'' methodology. The REPL approach, pioneered by the Lisp programming language and now prevalent in interpreted languages such as Python, Julia, and R, revolves around interactive exploration. Developers iteratively refine snippets of code in a REPL or notebook environment~\bibnote{\url{https://jupyter.org/}}, using examples and tests to validate each step. Because of the dynamic nature of the languages, very little can be automatically inferred about the code. Extensive testing is therefore required to facilitate any kind of refactoring or optimization. Iversonian languages such as APL, J, Q, K, and BQN also encourage REPL-driven development, but take a different approach for achieving correctness. In these languages, code is treated like a mathematical equation and extreme conciseness is encouraged if not enforced. For instance, an implementation of Conway's Game of Life in K may look like this~\bibnote{\url{https://thesweeheng.wordpress.com/2009/02/10/game-of-life-in-one-line-of-k/}}: \verb|life:{3=a-x*4=a:2{+(0+':x)+1_x,0}/x}|. Correctness is often self-evident from the composition of a few well-defined primitives and the shortness of the implementation.

Conversely, the "think-first" approach prioritizes strong typing and rich type systems, as found in languages such as Haskell, Futhark, and Dex. The compiler becomes a development partner, with types serving as constraints that guide the programmer toward correct implementations. The principle is that well-typed simple programs are likely to be correct programs. Neither approach is universally superior; the optimal choice often depends on the specific task, team preferences, and individual working styles. For example, visual tasks such as plotting benefit from interactive environments, while memory management issues are better addressed through careful up-front design.

There are situations where existing kernels prove inadequate for implementing certain algorithms, making the ability to develop custom, high-performance kernels essential. Traditionally, languages such as Fortran and C++ have served as the workhorses for performance-critical numerical code. For example, major density functional theory (DFT) packages such as VASP~\cite{Kresse1,Kresse2} and Quantum Espresso~\cite{Gianno_2009_QUANTUM_ESPRESS} are written in Fortran, while TRIQS~\cite{Parcol_2015_TRIQS_A_toolbo}, a modern package for the Green's functions formalism, is implemented primarily in C++. However, these low-level languages often impede rapid algorithm development and face challenges in achieving performance portability across different hardware architectures. In particular, writing kernels that take advantage of hardware features such as SIMD instructions on modern CPUs requires platform-specific implementations, resulting in significant maintenance overhead and increased testing complexity. To address these challenges, just-in-time (JIT) compilation is gaining popularity in heterogeneous computing. JIT compilation enables run-time generation of optimized kernels tailored to specific hardware architectures, allowing developers to use higher-level languages for kernel development. This trend is exemplified by a number of so-called domain-specific languages (DSLs) embedded in Python, such as Halide, TVM, Triton, and Pallas. These DSLs provide powerful abstractions for building and tuning high-performance kernels for CPUs, GPUs, and TPUs (Tensor Processing Units~\bibnote{\url{https://cloud.google.com/tpu}}), all while maintaining a Python-centric ecosystem.

As computing requirements grow, scaling to multiple compute nodes becomes essential. This is most often achieved using the Single Program Multiple Data (SPMD) paradigm, where identical copies of the same program run on multiple processors simultaneously, operating on different parts of the data. Transforming a program written for shared-memory (single-node) parallelism into an SPMD distributed-memory (multi-node) implementation requires significant algorithm restructuring. This is because the single flow of control and data is lost when moving to multiple nodes. For example, developers must think in terms of per-node elements rather than whole arrays. Global operations like reductions and scans become multi-level processes where a per-node operation is performed first, followed by an internode communication step.

In practice, most communication in SPMD programs is implemented using the Message Passing Interface (MPI) standard~\cite{mpi41}. As the name suggests, processes communicate by sending messages where, typically\footnote{MPI-3 has added support for so-called one-sided communication, but most code does not utilize it.}, both the sender and the receiver must explicitly invoke functions for communication to occur (e.g., \verb|MPI_Send|/\verb|MPI_Recv| pair). The Partitioned Global Address Space (PGAS) paradigm offers an alternative approach for expressing communication in distributed-memory systems. It introduces the concept of a global address space where every node can directly reference any portion of the data. Data stored on another node is accessed using remote memory accesses (RMA), which have hardware implementations in modern networks such as InfiniBand~\bibnote{\url{https://www.infinibandta.org/about-infiniband/}} and HPE Slingshot~\cite{De_Sen_2020_An_In_Depth_Ana}. Depending on the implementation, RMA function calls can be very explicit (e.g., \verb|upcxx::rget|/\verb|upcxx::rput| in UPC++) or completely hidden from the programmer by overloading array element/slice access functions. Beyond RMA, some libraries such as UPC++ and languages such as Chapel and X10 also support remote procedure calls (RPCs), which are implemented efficiently via so-called active messages.

Although most PGAS implementations follow the SPMD paradigm, Chapel and X10 programming languages support a ``global view'' abstraction. In this model, the \verb|main| function initially executes on just one process, with both distributed- and shared-memory parallelism expressed through tasks. With these capabilities, a shared-memory parallel algorithm implemented in Chapel requires only minor modifications to run on multiple nodes. While achieving optimal performance may still require refactoring, this can be done iteratively, making Chapel particularly suitable for rapid prototyping. Chapel excels at balancing high-level features (such as task-based parallelism, distributed iterators, and built-in distributed arrays) with low-level features (such as raw pointer access and remote direct memory access (RDMA) calls). Chapter~\ref{ch:paw-atm} presents a practical example where a Chapel-based implementation significantly outperforms a previous state-of-the-art MPI-based solution, demonstrating Chapel's potential for complex physics simulations.

% As computing requirements grow, scaling to multiple compute nodes becomes essential. This is most often achieved using the Single Program Multiple Data (SPMD) paradigm, where identical copies of the same program run on multiple processors simultaneously, operating on different parts of the data and communicating through message passing via the Message Passing Interface (MPI) standard~\cite{mpi41}. Transforming a program written for shared-memory (single-node) parallelism into a distributed-memory (multi-node) implementation that utilizes MPI efficiently is a non-trivial task that often requires a complete rewrite of the algorithm. The Partitioned Global Address Space (PGAS) paradigm offers an alternative approach for programming distributed-memory systems. It introduces the concept of a global address space where every node can directly reference any portion of the data. Accessing data stored on another node is performed using remote memory accesses (RMA), which have hardware implementations in modern networks such as InfiniBand~\bibnote{\url{https://www.infinibandta.org/about-infiniband/}} and HPE Slingshot~\cite{De_Sen_2020_An_In_Depth_Ana}. The communication can be hidden behind a library or language, allowing the programmer to focus on the algorithmic details while the runtime automatically issues remote memory access calls. Beyond RMA, some libraries such as UPC++ and languages such as Chapel allow remote procedure calls (RPCs) implemented efficiently via so-called active messages. Furthermore, Chapel and X10 programming languages support a so-called global view abstraction where the \verb|main| function initially executes on just one process, with both distributed- and shared-memory parallelism expressed through tasks. With these capabilities in place, a shared-memory parallel algorithm implemented in Chapel requires only minor modifications to run on multiple nodes. Although achieving optimal performance may still require refactoring, this can be done iteratively, making Chapel particularly suitable for rapid prototyping. Chapel excels at balancing high-level features (such as task-based parallelism, distributed iterators, and built-in distributed arrays) with low-level features (such as raw pointer access and remote direct memory access (RDMA) calls). Chapter~\ref{ch:paw-atm} presents a practical example where a Chapel-based implementation significantly outperforms a previous state-of-the-art MPI-based solution, demonstrating Chapel's potential for complex physics simulations.

Beyond performance and scalability, reproducibility is a critical but often overlooked aspect of numerical simulations in physics. Unlike software development in a commercial setting, physics simulations are often developed as one-off projects. Returning to a codebase months or years later to reproduce results or make changes can prove challenging. Although containerization technologies such as Docker and Singularity are gaining traction to address the reproducibility problem in HPC and ML, alternative technologies such as Nix~\bibnote{\url{https://nixos.org/}} offer promising solutions. As a demonstration, this entire thesis, including all figures based on research data, can be rebuilt from raw data with a single Nix command: \texttt{nix build github:twesterhout/phd-thesis}.

\section{Application to quantum many-body physics}

Quantum many-body physics is a rapidly developing field of condensed matter physics that lies at the interface between fundamental quantum mechanics and the emergent collective behavior of interacting particles. It is low-level enough for quantum mechanics to play a crucial role, yet high-level enough to allow the computation of useful material properties. Quantum many-body theory allows the description and understanding of a wide range of beautiful phenomena, including magnetism, superconductivity, and exotic phases of matter such as quantum spin liquids~\cite{Zhou_2017_Quantum_spin_li}. Progress in this field is driven by a healthy collaboration between theory and experiment. Advances in cold atom experiments~\cite{Gross_2017_Quantum_simulat}, Rydberg-based quantum simulators~\cite{Browae_2020_Many_body_physi}, and superconducting qubit-based quantum computers~\cite{Anders_2025_Thermalization}, among others, provide a versatile experimental playground for observing quantum many-body phenomena.

As mentioned earlier, the purpose of this work is to show how recent advances in computing can improve simulations in quantum many-body physics. To this end, we consider one of the most challenging problems in the field---the many-body electron problem, which arises due to the complex interactions between electrons in a system. In contrast to the single-particle picture, where electrons are treated as independent entities, the many-body problem requires accounting for the correlations between electrons. The difficulty of this problem has led some to question whether it is even solvable in principle, elevating it to a philosophical dilemma. Despite the daunting challenges posed by the many-body electron problem, there are limits in which innovative approaches have been developed to tackle it.

In the limit of weak electron-electron interactions, the Coulomb interaction can be treated as a perturbation, allowing calculations to be performed effectively in the single-particle picture using perturbation theory. The random phase approximation (RPA)~\cite{Vonsov2012QuantumSolidS,giuliani2005quantum} is one such approach that allows the study of plasmonic effects. RPA is typically applied to homogeneous electron systems and formulated in momentum space. For spatially inhomogeneous systems, such as nanoelectronic devices, a momentum space representation is inapplicable, necessitating the use of real-space RPA. These systems are of particular interest for practical applications in plasmonics. Numerically, real-space RPA involves the evaluation of the so-called Lindhard function, which is computationally demanding. In Chapters~\ref{ch:prb18} and \ref{ch:mat22}, we demonstrate how an efficient implementation allows us to study realistic systems. Chapter~\ref{ch:prb18} starts with a low-level C++ implementation of the Lindhard function based on Basic Linear Algebra Subprograms (BLAS)~\cite{Lawson_1979_Basic_Linear_Al} primitives. This allows the simulation of systems with up to 4000 lattice sites, which was considered state-of-the art. This proved sufficient to consider an interesting case of translational symmetry breaking---fractal lattices in two dimensions. These lattices have recently become experimentally realizable~\cite{assembly2015}, but an analysis of their plasmonic properties has been lacking. Chapter~\ref{ch:prb18} fills this gap and describes a curious relation between the ramification number of a fractal and its plasmonic properties.

Another material that has received a lot of attention is twisted bilayer graphene (TBLG)~\cite{Andrei_2020_Graphene_bilaye}. Due to its tunability, it has proven to be a fruitful experimental and theoretical playground. Chapter~\ref{ch:mat22} investigates plasmonic effects in TBLG at twist angles away from the magic angle, where electron-electron interactions are still small enough to be treated within the RPA framework. Utilizing the techniques of Section~\ref{sec:intro:data-oriented}, Chapter~\ref{ch:mat22} improves upon the implementation of Chapter~\ref{ch:prb18}. The problem is formulated as a series of matrix-matrix multiplications, allowing full utilization of the available hardware. In contrast to Chapter~\ref{ch:prb18}, which required tens of compute nodes, all of Chapter~\ref{ch:mat22}'s calculations are performed on a single Nvidia V100S GPU. This efficiency allows us to experiment freely with model parameters and to consider an \textit{ab initio}-derived material-realistic model for the Coulomb interaction, surpassing the effective models employed in prior studies. Chapter~\ref{ch:mat22} discovers an intriguing phenomenon we call plasmonic quantum dots, which bear resemblance to electronic atomic orbitals.

However, considering smaller twist angles than those in Chapter~\ref{ch:mat22} poses a challenge. The appearance of a flat band near the Fermi surface indicates strong electron-electron interactions, rendering perturbative treatments inadequate. In this regime, non-perturbative methods should be used. Exact diagonalization (ED) is a robust method for the exact solution of the quantum many-body problem. Although ED involves large matrices and its computational time scales exponentially, restricting it to small system sizes, it remains one of the best methods for the initial analysis of a system and as a benchmark for other computational methods. By exploiting the symmetries of the Hamiltonian, the computational complexity can be reduced and the accessible system size slightly increased. By taking full advantage of modern processor capabilities and multiple programming languages, Chapters~\ref{ch:ls21} and \ref{ch:paw-atm} present an implementation of a state-of-the-art ED package that achieves nearly an order of magnitude speedup over existing packages while maintaining ease of use. Chapter~\ref{ch:corr-hops} presents an example analysis that can be performed using ED methods, discussing the technicalities arising from flat bands, their connection to a phenomenon called Nagaoka magnetism~\cite{Nagaok_1966_Ferromagnetism}, and how to handle them carefully.

Finally, some phenomena are not observable on the small length scales accessible to ED, such as the spin liquid phases that arise in some frustrated magnets. The study of these models often requires the analysis of the long-range behavior of the correlation functions, which is hindered by finite size effects in ED. In these cases, quantum Monte Carlo~\cite{Sandvi_1991_Quantum_Monte_C} or variational methods can be used. The former would be the preferred method as it provides an unbiased estimator of the desired properties; however, in many relevant cases it suffers from the so-called sign problem, which causes its statistical errors to grow exponentially with the system size and inverse temperature~\cite{NP_Troyer}. To study the low-temperature behavior of frustrated magnets, variational methods remain the method of choice. Variational methods typically involve creating a parametrized ansatz for the wave function and then optimizing the parameters through, e.g., energy minimization. Since the wave function provides a complete description of a pure quantum system, all physical properties can be extracted from the ansatz. Recently, neural networks have emerged as a promising, expressive, and unbiased ansatz. Chapter~\ref{ch:nc20} studies the so-called neural quantum states (NQS) and shows that an analogue of the sign problem arises for frustrated quantum systems, so similar that some refer to it as the sign problem itself. Chapter~\ref{ch:cp23} develops an alternative formulation of the problem, analyzes it from the perspective of spin glasses, and proposes a possible solution to the challenge of learning the signs of the wave function coefficients. Thus, Chapters~\ref{ch:nc20} and \ref{ch:cp23} identify the central challenge faced by variational methods for frustrated magnets, an insight made possible by the standard machine learning analysis in Chapter~\ref{ch:nc20}.
